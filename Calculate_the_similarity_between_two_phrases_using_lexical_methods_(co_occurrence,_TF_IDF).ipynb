{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "REYls9kPUTMI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nt3_zZtDvZlH"
      },
      "outputs": [],
      "source": [
        "train_file_path = 'train.txt'\n",
        "test_file_path = 'test.txt'\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    pairs = [line.strip().split('\\t') for line in lines]\n",
        "    phrases1, phrases2, similarities = zip(*pairs)\n",
        "    return phrases1, phrases2, similarities\n",
        "\n",
        "train_phrases1, train_phrases2, train_similarities = load_data(train_file_path)\n",
        "test_phrases1, test_phrases2, test_similarities = load_data(test_file_path)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'Phrase1': train_phrases1, 'Phrase2': train_phrases2, 'Similarity': train_similarities})\n",
        "test_data = pd.DataFrame({'Phrase1': test_phrases1, 'Phrase2': test_phrases2, 'Similarity': test_similarities})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJDAmMYjxgTt",
        "outputId": "f863104d-46f0-423d-f9d3-7810de9fed6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13365 entries, 0 to 13364\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Phrase1     13365 non-null  object\n",
            " 1   Phrase2     13365 non-null  object\n",
            " 2   Similarity  13365 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 313.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Co-Occurence**"
      ],
      "metadata": {
        "id": "4NoiuOUYUd5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzzmhF4HzExN",
        "outputId": "6248de45-5a9e-4659-aca9-ca42349e6909"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import euclidean\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "def calculate_similarity_co_occurrence(sentence1, sentence2):\n",
        "    # Tokenize sentences\n",
        "    X_list = word_tokenize(sentence1)\n",
        "    Y_list = word_tokenize(sentence2)\n",
        "\n",
        "    # Remove stopwords\n",
        "    sw = stopwords.words('english')\n",
        "    X_list_no_sw = [w.lower() for w in X_list if not w.lower() in sw]\n",
        "    Y_list_no_sw = [w.lower() for w in Y_list if not w.lower() in sw]\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    X_lem_set = {lemmatizer.lemmatize(w.lower()) for w in X_list_no_sw}\n",
        "    Y_lem_set = {lemmatizer.lemmatize(w.lower()) for w in Y_list_no_sw}\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    l1 = [1 if w in X_lem_set else 0 for w in (X_lem_set.union(Y_lem_set))]\n",
        "    l2 = [1 if w in Y_lem_set else 0 for w in (X_lem_set.union(Y_lem_set))]\n",
        "    cosine_sim = cosine_similarity(np.array(l1).reshape(1, -1), np.array(l2).reshape(1, -1))\n",
        "\n",
        "    # Calculate Euclidean distance\n",
        "    euclidean_dist = euclidean(np.array(l1), np.array(l2))\n",
        "\n",
        "    # Calculate Jaccard similarity\n",
        "    jaccard_sim = jaccard_score(l1, l2)\n",
        "\n",
        "    return cosine_sim[0][0], euclidean_dist, jaccard_sim\n"
      ],
      "metadata": {
        "id": "HMYgXRNS0BcF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**"
      ],
      "metadata": {
        "id": "dFs5M0vmUqvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from scipy.spatial.distance import euclidean\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def lemmatize_sentence(sentence):\n",
        "    # Initialize the WordNet lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(sentence)\n",
        "\n",
        "    # Lemmatize each word in the sentence\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join the lemmatized words back into a sentence\n",
        "    #lemmatized_sentence = ' '.join(lemmatized_words)\n",
        "\n",
        "    return lemmatized_words\n",
        "def calculate_similarity_TF_IDF(sentence1, sentence2) :\n",
        "    list_sentences=[sentence1, sentence2]\n",
        "    tfidf_vectorizer = TfidfVectorizer(tokenizer=lemmatize_sentence, stop_words='english')\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(list_sentences)\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    tfidf_df = tfidf_matrix.toarray()\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
        "    vec1 = tfidf_matrix[0].toarray().flatten() if hasattr(tfidf_matrix[0], 'toarray') else tfidf_matrix[0].flatten()\n",
        "    vec2 = tfidf_matrix[1].toarray().flatten() if hasattr(tfidf_matrix[1], 'toarray') else tfidf_matrix[1].flatten()\n",
        "    intersection = np.sum(np.minimum(vec1, vec2))\n",
        "    union = np.sum(np.maximum(vec1, vec2))\n",
        "    jaccard_sim = intersection / union\n",
        "    euclidean_dist = euclidean(tfidf_matrix[0].toarray().flatten(), tfidf_matrix[1].toarray().flatten())\n",
        "    return cosine_sim, euclidean_dist, jaccard_sim\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R-s9R3kW5vo5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Cosine Co-occurrence', 'Euclidean Co-occurrence', 'Jaccard Co-occurrence']] = df.apply(lambda x: calculate_similarity_co_occurrence(x['Phrase1'], x['Phrase2']), axis=1, result_type='expand')\n",
        "df[['Cosine TF-IDF', 'Euclidean TF-IDF', 'Jaccard TF-IDF']] = df.apply(lambda x: calculate_similarity_TF_IDF(x['Phrase1'], x['Phrase2']), axis=1, result_type='expand')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69GY3Agw3FmJ",
        "outputId": "af92ca93-1d39-4e86-cc01-cee92aaf8a47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl9MPw_n3Hwf",
        "outputId": "07e3bd5d-4bc0-487c-9d7e-406f61294420"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13365 entries, 0 to 13364\n",
            "Data columns (total 9 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   Phrase1                  13365 non-null  object \n",
            " 1   Phrase2                  13365 non-null  object \n",
            " 2   Similarity               13365 non-null  object \n",
            " 3   Cosine Co-occurrence     13365 non-null  float64\n",
            " 4   Euclidean Co-occurrence  13365 non-null  float64\n",
            " 5   Jaccard Co-occurrence    13365 non-null  float64\n",
            " 6   Cosine TF-IDF            13365 non-null  object \n",
            " 7   Euclidean TF-IDF         13365 non-null  float64\n",
            " 8   Jaccard TF-IDF           13365 non-null  float64\n",
            "dtypes: float64(5), object(4)\n",
            "memory usage: 939.9+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Data**"
      ],
      "metadata": {
        "id": "oJNJSItUUw82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAkNyV5cPRMf",
        "outputId": "34d24628-1bbb-4eb0-9887-6cd23fbe794a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 250 entries, 0 to 249\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Phrase1     250 non-null    object\n",
            " 1   Phrase2     250 non-null    object\n",
            " 2   Similarity  250 non-null    object\n",
            "dtypes: object(3)\n",
            "memory usage: 6.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[['Cosine Co-occurrence', 'Euclidean Co-occurrence', 'Jaccard Co-occurrence']] = test_data.apply(lambda x: calculate_similarity_co_occurrence(x['Phrase1'], x['Phrase2']), axis=1, result_type='expand')\n",
        "test_data[['Cosine TF-IDF', 'Euclidean TF-IDF', 'Jaccard TF-IDF']] = test_data.apply(lambda x: calculate_similarity_TF_IDF(x['Phrase1'], x['Phrase2']), axis=1, result_type='expand')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpdCRwlDPflc",
        "outputId": "0e6f3142-6845-4933-85d5-73799061662b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df[['Cosine Co-occurrence', 'Euclidean Co-occurrence', 'Jaccard Co-occurrence',\n",
        "              'Euclidean TF-IDF', 'Jaccard TF-IDF']]\n",
        "y_train = df['Similarity']\n",
        "\n",
        "X_test = test_data[['Cosine Co-occurrence', 'Euclidean Co-occurrence', 'Jaccard Co-occurrence',\n",
        "                    'Euclidean TF-IDF', 'Jaccard TF-IDF']]\n",
        "y_test = test_data['Similarity']"
      ],
      "metadata": {
        "id": "9FnbOu8d_v6F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LinearRegression**"
      ],
      "metadata": {
        "id": "IL5BAOBWU9lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_test_lr = lr_model.predict(X_test)\n",
        "mse_test_lr = mean_squared_error(y_test, y_pred_test_lr)\n",
        "\n",
        "print(\"Linear Regression Testing Mean Squared Error:\", mse_test_lr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1fjnvnjU1rc",
        "outputId": "b652c472-85ba-473b-81fb-bff68dd6687e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Testing Mean Squared Error: 0.9909606450181514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RandomForestRegressor**"
      ],
      "metadata": {
        "id": "oBstsPOeVYb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred_test_rf = rf_model.predict(X_test)\n",
        "mse_test_rf = mean_squared_error(y_test, y_pred_test_rf)\n",
        "\n",
        "print(\"Random Forest Testing Mean Squared Error:\", mse_test_rf)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fvw_0ojVArz",
        "outputId": "aa85bbd8-faa6-4ff0-b3e9-74fc2c4621c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Testing Mean Squared Error: 1.1910818757046437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GradientBoostingRegressor**"
      ],
      "metadata": {
        "id": "XhJLy92WVbnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_test_gb = gb_model.predict(X_test)\n",
        "mse_test_gb = mean_squared_error(y_test, y_pred_test_gb)\n",
        "\n",
        "\n",
        "print(\"Gradient Boosting Testing Mean Squared Error:\", mse_test_gb)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX7etC18VV9y",
        "outputId": "8ac40d93-a7d6-4353-8afa-f7de9f0d3f98"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Testing Mean Squared Error: 0.9558532147046316\n"
          ]
        }
      ]
    }
  ]
}